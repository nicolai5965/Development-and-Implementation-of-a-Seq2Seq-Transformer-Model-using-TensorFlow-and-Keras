{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOPNtt9g0JSLUDxW5t+aQby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolai5965/Transformer_scratch_tensorflow/blob/main/Load_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dukOn2JAyVsc",
        "outputId": "3663b37d-a17f-45e4-f943-33296bc57aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Import Natural Language Toolkit and download required corpora\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import Tensorflow and Keras related libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Import Google Colab library for Google Drive integration\n",
        "from google.colab import drive\n",
        "# Mount Google Drive to load the dataset\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTokenizer:\n",
        "    # Initialize the tokenizer\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit the tokenizer to the texts and add Start and End of Sentence tokens\n",
        "    def fit(self, texts):\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "        self.tokenizer.word_index['<SOS>'] = len(self.tokenizer.word_index) + 1\n",
        "        self.tokenizer.word_index['<EOS>'] = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "    # Tokenize the texts\n",
        "    def tokenize(self, texts):\n",
        "        return [self._custom_tokenize_text(t) for t in texts]\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    def texts_to_sequences(self, texts):\n",
        "        return self.tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Custom tokenize text method that also retains punctuation\n",
        "    def _custom_tokenize_text(self, text):\n",
        "        return re.findall(r'\\b\\w+\\b|[' + string.punctuation + ']', text)\n",
        "\n",
        "    # Getter method for word index\n",
        "    @property\n",
        "    def word_index(self):\n",
        "        return self.tokenizer.word_index\n",
        "\n",
        "    # Getter method for index word\n",
        "    @property\n",
        "    def index_word(self):\n",
        "        return self.tokenizer.index_word"
      ],
      "metadata": {
        "id": "OueLYruPDlNy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_attention_heads, name=\"multi_head_attention\", **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(name=name, **kwargs)\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.d_model = d_model\n",
        "        self.scaled_dot_product_attention = ScaledDotProductAttention()\n",
        "\n",
        "        self.depth = d_model // self.num_attention_heads  # Calculate depth for each head\n",
        "\n",
        "        # Initializing linear transformation layers\n",
        "        self.query_lin = tf.keras.layers.Dense(d_model, name=name+\"_query_lin\")  # Linear transformation layer for queries\n",
        "        self.key_lin = tf.keras.layers.Dense(d_model, name=name+\"_key_lin\")  # Linear transformation layer for keys\n",
        "        self.value_lin = tf.keras.layers.Dense(d_model, name=name+\"_value_lin\")  # Linear transformation layer for values\n",
        "\n",
        "        self.final_lin = tf.keras.layers.Dense(d_model, name=name+\"_final_lin\")  # Linear transformation layer for final output\n",
        "\n",
        "    # Function to split input into multiple heads\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.depth))  # Reshape input tensor to split the last dimension\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # Transpose the tensor dimensions\n",
        "\n",
        "    # Function for forward propagation\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]  # Get the batch size\n",
        "\n",
        "        # Apply linear transformations\n",
        "        W_q = self.query_lin(q)  # Linear transformation for queries\n",
        "        W_k = self.key_lin(k)  # Linear transformation for keys\n",
        "        W_v = self.value_lin(v)  # Linear transformation for values\n",
        "\n",
        "        # Apply scaled dot product attention\n",
        "        q_split = self.split_heads(W_q, batch_size)  # Split queries into multiple heads\n",
        "        k_split = self.split_heads(W_k, batch_size)  # Split keys into multiple heads\n",
        "        v_split = self.split_heads(W_v, batch_size)  # Split values into multiple heads\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
        "            q_split, k_split, v_split, mask)  # Call to scaled dot product attention layer\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # Transpose attention output\n",
        "\n",
        "        # Reshape and apply final linear transformation\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "        output = self.final_lin(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    # Function to get layer's configuration\n",
        "    def get_config(self):\n",
        "        config = super(MultiHeadAttention, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_attention_heads': self.num_attention_heads,\n",
        "            'name': self.name,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    # Classmethod to create layer from its config\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ],
      "metadata": {
        "id": "KFj26Vkv0Bhq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "    def __init__(self):\n",
        "        # Initialize the loss function with specific parameters\n",
        "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def compute(self, real, pred):\n",
        "        # Exclude padding for the computation of the loss\n",
        "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "        loss_ = self.loss_object(real, pred)\n",
        "\n",
        "        # Convert mask to the same type as loss\n",
        "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "        # Apply the mask to the loss\n",
        "        loss_ *= mask\n",
        "\n",
        "        # Return the mean loss\n",
        "        return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        # Cast the model size and warmup steps to float32 for later computations\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Cast the step to float32\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        # Compute the learning rate as per the formula\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        # Override the get_config method to include additional arguments\n",
        "        return {\n",
        "            \"d_model\": float(self.d_model.numpy()),\n",
        "            \"warmup_steps\": float(self.warmup_steps.numpy())\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Override the from_config method to handle custom arguments\n",
        "        return cls(**config)"
      ],
      "metadata": {
        "id": "cc5Zj8uIynuP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Transformer_SentimentAnalysis_15_epochs_0.08_data_20230718_1053_v1_0.09acc' # Transformer_SentimentAnalysis_50_epochs_0.08_data_20230717_0945_v1_0.35acc\n",
        "\n",
        "# Define the model path\n",
        "model_path = f'/content/drive/MyDrive/Colab Notebooks/Machine Learning/TensorFlow/Transformer/Transformer_Weight/{model_name}'\n",
        "\n",
        "loss_function = LossFunction()\n",
        "\n",
        "model_saved = tf.keras.models.load_model(\n",
        "    model_path,\n",
        "    custom_objects={\n",
        "        \"CustomSchedule\": CustomSchedule,\n",
        "        \"compute\": loss_function.compute,\n",
        "        \"MultiHeadAttention\": MultiHeadAttention\n",
        "    }\n",
        ")\n",
        "model_saved.summary()\n"
      ],
      "metadata": {
        "id": "FCef2ItKHDhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7cf373-d9e2-4be0-ef44-165d02797435"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_lam_shape (Lambda)     ()                   0           ['decoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " padding_mask_decoder (Lambda)  (None, None)         0           ['decoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " padding_mask_encoder (Lambda)  (None, None)         0           ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " look_ahead_mask_ones_matrix (L  (None, None)        0           ['decoder_lam_shape[0][0]']      \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " padding_mask_expanded_decoder   (None, 1, 1, None)  0           ['padding_mask_decoder[0][0]']   \n",
            " (Lambda)                                                                                         \n",
            "                                                                                                  \n",
            " padding_mask_expanded_encoder   (None, 1, 1, None)  0           ['padding_mask_encoder[0][0]']   \n",
            " (Lambda)                                                                                         \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, None)         0           ['look_ahead_mask_ones_matrix[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " Transformer_Block (Transformer  (None, None, 25198)  67180378   ['encoder_input[0][0]',          \n",
            " )                                                                'padding_mask_expanded_decoder[0\n",
            "                                                                 ][0]',                           \n",
            "                                                                  'padding_mask_expanded_encoder[0\n",
            "                                                                 ][0]',                           \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'decoder_input[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 67,180,378\n",
            "Trainable params: 67,180,378\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first layer of the model\n",
        "first_layer = model_saved.layers[-1]\n",
        "\n",
        "# Print out the configuration of the first layer\n",
        "print(first_layer.get_config())\n"
      ],
      "metadata": {
        "id": "ZtqKb_xAaM-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bca58a-1657-4a80-dda3-98cc0d47fbd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Transformer_Block', 'trainable': True, 'dtype': 'float32', 'enc_num_layers': 6, 'dec_num_layers': 6, 'd_model': 510, 'enc_num_attention_heads': 6, 'dec_num_attention_heads': 6, 'enc_dim_ff': 800, 'dec_dim_ff': 800, 'input_vocab_size': 25198, 'target_vocab_size': 25198, 'pe_input': 60, 'pe_target': 60, 'enc_rate': 0.14, 'dec_rate': 0.16, 'verbose': False, 'encoder': {'class_name': 'Encoder', 'config': {'name': 'Encoder', 'trainable': True, 'dtype': 'float32', 'num_layers': 6, 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'input_vocab_size': 25198, 'max_position': 60, 'rate': 0.14, 'embedding_layer': {'class_name': 'MyEmbeddingLayer', 'config': {'name': 'encoder_embedding', 'trainable': True, 'dtype': 'float32', 'vocab_size': 25198, 'd_model': 510, 'training': True, 'verbose': False}}, 'pos_encoding_layer': {'class_name': 'PositionalEncoding', 'config': {'name': 'encoder_encoding', 'trainable': True, 'dtype': 'float32', 'position': 60, 'd_model': 510}}, 'encoder_layers': [{'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 9}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 10}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 11}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 12}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_2', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 13}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 14}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_3', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 15}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 16}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_3', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 17}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 18}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 19}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 20}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_4', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_3', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_6', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 21}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 22}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_7', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 23}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 24}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_5', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_4', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_8', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 25}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 26}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_9', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 27}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 28}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'EncoderLayer', 'config': {'name': 'encoder_layer_6', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.14, 'mha': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_encoder', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_5', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'encoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_10', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 29}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 30}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'encoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_11', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 31}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 32}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}]}}, 'decoder': {'class_name': 'Decoder', 'config': {'name': 'Decoder', 'trainable': True, 'dtype': 'float32', 'num_layers': 6, 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'target_vocab_size': 25198, 'max_position': 60, 'rate': 0.16, 'embedding_layer': {'class_name': 'MyEmbeddingLayer', 'config': {'name': 'decoder_embedding', 'trainable': True, 'dtype': 'float32', 'vocab_size': 25198, 'd_model': 510, 'training': True, 'verbose': False}}, 'pos_encoding': {'class_name': 'PositionalEncoding', 'config': {'name': 'decoder_encoding', 'trainable': True, 'dtype': 'float32', 'position': 60, 'd_model': 510}}, 'dec_layers': [{'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_6', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_12', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 33}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 34}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_13', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 35}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 36}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_14', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 37}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 38}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_7', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_15', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 39}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 40}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_16', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 41}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 42}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_17', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 43}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 44}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_3', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_8', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_18', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 45}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 46}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_19', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 47}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 48}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_20', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 49}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 50}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_4', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_9', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_21', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 51}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 52}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_22', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 53}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 54}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_23', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 55}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 56}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_5', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_10', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_24', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 57}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 58}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_25', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 59}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 60}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_26', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 61}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 62}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}, {'class_name': 'DecoderLayer', 'config': {'name': 'decoder_layer_6', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6, 'dim_ff': 800, 'rate': 0.16, 'mha1': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_1', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'mha2': {'class_name': 'MultiHeadAttention', 'config': {'name': 'MHA_decoder_2', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'num_attention_heads': 6}}, 'ffn': {'class_name': 'PointWiseFeedForwardNetwork', 'config': {'name': 'point_wise_feed_forward_network_11', 'trainable': True, 'dtype': 'float32', 'd_model': 510, 'dim_ff': 800, 'identifier': 'decoder'}}, 'norm_and_add1': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_1', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_27', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 63}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 64}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add2': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_2', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_28', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 65}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 66}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}, 'norm_and_add3': {'class_name': 'NormAndAdd', 'config': {'name': 'decoder_NaA_3', 'trainable': True, 'dtype': 'float32', 'layernorm': {'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_29', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 1e-06, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 67}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}, 'shared_object_id': 68}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}}}}], 'dropout': {'class_name': 'Dropout', 'config': {'name': 'dropout_31', 'trainable': True, 'dtype': 'float32', 'rate': 0.16, 'noise_shape': None, 'seed': None}}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "# Access a layer by name\n",
        "layer = model_saved.get_layer('Transformer_Block')\n",
        "\n",
        "# Print out the configuration of the layer\n",
        "#pprint.pprint(layer.get_config())\n",
        "\n"
      ],
      "metadata": {
        "id": "sRmfSIDRaeOD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a layer by name\n",
        "layer = model_saved.get_layer('Transformer_Block')\n",
        "\n",
        "# Get the layer configuration\n",
        "config = layer.get_config()\n",
        "\n",
        "# Get the encoder's and decoder's configuration\n",
        "encoder_config = config['encoder']['config']\n",
        "decoder_config = config['decoder']['config']\n",
        "\n",
        "# Convert these configurations into pandas DataFrames\n",
        "encoder_config_df = pd.DataFrame.from_dict(encoder_config, orient='index', columns=['Value'])\n",
        "decoder_config_df = pd.DataFrame.from_dict(decoder_config, orient='index', columns=['Value'])\n",
        "\n",
        "# Print the DataFrames\n",
        "print(\"Encoder Configuration:\")\n",
        "print(encoder_config_df)\n",
        "print(\"\\nDecoder Configuration:\")\n",
        "print(decoder_config_df)\n"
      ],
      "metadata": {
        "id": "mtpbmuCwo2lU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e289ef35-1ba6-4862-e66e-938d69d5af1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Configuration:\n",
            "                                                                 Value\n",
            "name                                                           Encoder\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "num_layers                                                           6\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "input_vocab_size                                                 25198\n",
            "max_position                                                        60\n",
            "rate                                                              0.14\n",
            "embedding_layer      {'class_name': 'MyEmbeddingLayer', 'config': {...\n",
            "pos_encoding_layer   {'class_name': 'PositionalEncoding', 'config':...\n",
            "encoder_layers       [{'class_name': 'EncoderLayer', 'config': {'na...\n",
            "\n",
            "Decoder Configuration:\n",
            "                                                                 Value\n",
            "name                                                           Decoder\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "num_layers                                                           6\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "target_vocab_size                                                25198\n",
            "max_position                                                        60\n",
            "rate                                                              0.16\n",
            "embedding_layer      {'class_name': 'MyEmbeddingLayer', 'config': {...\n",
            "pos_encoding         {'class_name': 'PositionalEncoding', 'config':...\n",
            "dec_layers           [{'class_name': 'DecoderLayer', 'config': {'na...\n",
            "dropout              {'class_name': 'Dropout', 'config': {'name': '...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the encoder's configuration\n",
        "encoder_config = config['encoder']['config']\n",
        "\n",
        "# Extract the encoder layers\n",
        "encoder_layers_config = encoder_config['encoder_layers']\n",
        "\n",
        "# For each layer in the encoder layers, convert its configuration into a DataFrame and print it\n",
        "for i, layer_config in enumerate(encoder_layers_config):\n",
        "    layer_config_df = pd.DataFrame.from_dict(layer_config['config'], orient='index', columns=['Value'])\n",
        "    print(f\"Encoder Layer {i + 1} Configuration:\")\n",
        "    print(layer_config_df)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "I6WUaBvI0NyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657df6a0-07b9-4668-ec80-d53de61e8767"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Layer 1 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_1\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n",
            "Encoder Layer 2 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_2\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n",
            "Encoder Layer 3 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_3\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n",
            "Encoder Layer 4 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_4\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n",
            "Encoder Layer 5 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_5\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n",
            "Encoder Layer 6 Configuration:\n",
            "                                                                 Value\n",
            "name                                                   encoder_layer_6\n",
            "trainable                                                         True\n",
            "dtype                                                          float32\n",
            "d_model                                                            510\n",
            "num_attention_heads                                                  6\n",
            "dim_ff                                                             800\n",
            "rate                                                              0.14\n",
            "mha                  {'class_name': 'MultiHeadAttention', 'config':...\n",
            "ffn                  {'class_name': 'PointWiseFeedForwardNetwork', ...\n",
            "norm_and_add1        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "norm_and_add2        {'class_name': 'NormAndAdd', 'config': {'name'...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading tokenizer"
      ],
      "metadata": {
        "id": "Ujdu8U8jD3EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "tokenizer_path = f'/content/drive/MyDrive/Colab Notebooks/Machine Learning/TensorFlow/Transformer/Transformer_Weight/{model_name}_tokenizer.pkl'\n",
        "def load_tokenizer(path):\n",
        "    # Load the tokenizer using pickle\n",
        "    with open(path, 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = load_tokenizer(tokenizer_path)"
      ],
      "metadata": {
        "id": "XJ0B8BUz0v01"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generator"
      ],
      "metadata": {
        "id": "qkL7sFb-Jc0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StochasticBeamSearch:\n",
        "    def __init__(self, model, tokenizer, beam_size=3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.beam_size = beam_size\n",
        "\n",
        "    def decode_sequence(self, sequence):\n",
        "        index_to_word = dict((i, word) for word, i in self.tokenizer.word_index.items())\n",
        "        # Exclude the '<EOS>' token in the final output\n",
        "        return ' '.join(index_to_word.get(token, '?') for token in sequence if index_to_word.get(token, '?') != '<EOS>')\n",
        "\n",
        "    def predict(self, start_sentence, max_length=10, temperature=1.0, repetition_penalty=0.5):\n",
        "        start_tokens = self.tokenizer.texts_to_sequences([start_sentence])\n",
        "        start_tokens = np.squeeze(start_tokens, axis=0)\n",
        "\n",
        "        # Initialize beam with the start tokens\n",
        "        beam = [(start_tokens, 0.0)]  # each element in the beam is (token_sequence, log_probability)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            for tokens, log_prob in beam:\n",
        "                # Predict next tokens for all current sequences in the beam\n",
        "                tokens = np.expand_dims(tokens, axis=0)\n",
        "                predictions = self.model.predict([tokens, tokens], verbose=0)\n",
        "\n",
        "                # Select the last token from predictions\n",
        "                predictions = predictions[0, -1, :]\n",
        "\n",
        "                # Apply temperature scaling\n",
        "                predictions /= temperature\n",
        "\n",
        "                # Get top k tokens and probabilities\n",
        "                top_k_probs, top_k_tokens = tf.math.top_k(predictions, k=self.beam_size)\n",
        "\n",
        "                # Form next candidates by adding new tokens to current sequences\n",
        "                for k in range(self.beam_size):\n",
        "                    if tokens[0][-1] != top_k_tokens[k]:\n",
        "                        updated_tokens = np.append(tokens, top_k_tokens[k])\n",
        "                        epsilon = 1e-9  # Small constant\n",
        "                        updated_log_prob = log_prob + np.log(top_k_probs[k].numpy() + epsilon)\n",
        "\n",
        "                        # Subtract repetition penalty for each repeated token\n",
        "                        token_counts = np.bincount(updated_tokens)\n",
        "                        repeated_token_count = len(token_counts[token_counts > 1])\n",
        "                        updated_log_prob -= repetition_penalty * repeated_token_count\n",
        "\n",
        "                        all_candidates.append((updated_tokens, updated_log_prob))\n",
        "\n",
        "            # Select new beam probabilistically\n",
        "            beam_probs = np.array([c[1] for c in all_candidates])\n",
        "            beam_probs = np.exp(beam_probs)  # Convert from log probabilities to probabilities\n",
        "\n",
        "            # Check for NaN values\n",
        "            if np.isnan(beam_probs).any():\n",
        "                print(\"NaN values detected in beam_probs. Replacing with uniform probabilities.\")\n",
        "                beam_probs = np.ones_like(beam_probs) / len(beam_probs)\n",
        "\n",
        "            # Make sure the probabilities sum up to 1\n",
        "            beam_probs = beam_probs / np.sum(beam_probs)\n",
        "            beam_indices = np.random.choice(range(len(beam_probs)), size=self.beam_size, p=beam_probs)\n",
        "\n",
        "            beam = [all_candidates[i] for i in beam_indices]\n",
        "\n",
        "            # Select the sequence with the highest probability from the final beam\n",
        "            tokens, _ = max(beam, key=lambda x: x[1])\n",
        "\n",
        "            # Check for EOS token and stop if found\n",
        "            if self.tokenizer.word_index['<EOS>'] in tokens:\n",
        "                break\n",
        "\n",
        "        # Decode tokens into text and return\n",
        "        return self.decode_sequence(tokens)\n"
      ],
      "metadata": {
        "id": "oQ8VkttMi9vp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search = StochasticBeamSearch(model_saved, tokenizer, beam_size=6)\n",
        "text = beam_search.predict(\"the sun dipped below the horizon casting a \", 50, temperature=1.3, repetition_penalty=0.3)\n",
        "print(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5-q2yMMjaTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b69ccbd-d2d3-4db9-d0b2-afeb44cc6fc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sun below the horizon casting a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Czwsym8zDHpa"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}